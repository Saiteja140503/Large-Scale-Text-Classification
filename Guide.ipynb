{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsHe3i8ka1MZ",
        "outputId": "d02add70-05ba-474c-e872-d03a85cec476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directories created successfully\n"
          ]
        }
      ],
      "source": [
        "# Create project structure\n",
        "import os\n",
        "\n",
        "project_dirs = [\n",
        "    'text-classification-service/src',\n",
        "    'text-classification-service/scripts',\n",
        "    'text-classification-service/model',\n",
        "    'text-classification-service/tests'\n",
        "]\n",
        "\n",
        "for dir_path in project_dirs:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "print('Directories created successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœ… PROJECT COMPLETED: Large-Scale Text Classification Service on AWS Lambda\n",
        "\n",
        "## ðŸ“„ Project Structure Created\n",
        "\n",
        "All files have been successfully created in Google Colab at `/content/text-classification-service/`\n",
        "\n",
        "### Core Components (13 files):\n",
        "\n",
        "**Infrastructure:**\n",
        "- `template.yaml` - AWS SAM CloudFormation template (Lambda + API Gateway + DynamoDB)\n",
        "\n",
        "**Lambda Function Code (src/):**\n",
        "- `lambda_handler.py` - Main handler for text classification requests\n",
        "- `model_loader.py` - S3 model loading with global caching\n",
        "- `logger.py` - DynamoDB async logging helper\n",
        "\n",
        "**ML & Training (scripts/):**\n",
        "- `train_model.py` - Model training using 20newsgroups dataset\n",
        "- `model/` directory (for text_classifier.pkl after training)\n",
        "\n",
        "**Testing:**\n",
        "- `tests/test_handler.py` - Unit tests for validation and error handling\n",
        "\n",
        "**Configuration:**\n",
        "- `requirements.txt` - Python dependencies (boto3, scikit-learn)\n",
        "- `.gitignore` - Git ignore rules\n",
        "\n",
        "**Documentation:**\n",
        "- `README.md` - Comprehensive project overview\n",
        "- `DEPLOYMENT_GUIDE.md` - Step-by-step deployment instructions\n",
        "- `swagger.yaml` - OpenAPI 3.0 specification\n",
        "- `CURL_EXAMPLES.sh` - Sample API test requests\n",
        "- `PROJECT_STRUCTURE.txt` - Detailed project structure\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸš€ Key Features\n",
        "\n",
        "âœ”ï¸ **Request Validation** - JSON, required text field, min 10 characters  \n",
        "âœ”ï¸ **ML Inference** - scikit-learn classification with confidence scores  \n",
        "âœ”ï¸ **Async Logging** - Fire-and-forget DynamoDB writes (non-blocking)  \n",
        "âœ”ï¸ **Error Handling** - 400/500 responses with clear error messages  \n",
        "âœ”ï¸ **Model Caching** - Global cache avoids S3 re-downloads on warm invocations  \n",
        "âœ”ï¸ **Serverless** - Auto-scaling, zero-idle costs, CloudWatch logs  \n",
        "âœ”ï¸ **Production-Ready** - OpenAPI docs, deployment guide, troubleshooting  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ› ï¸ Next Steps (AWS Deployment)\n",
        "\n",
        "1. **Train the model** (from local machine):\n",
        "   ```bash\n",
        "   cd text-classification-service/scripts\n",
        "   python train_model.py\n",
        "   cd ..\n",
        "   ```\n",
        "\n",
        "2. **Create S3 bucket & upload model**:\n",
        "   ```bash\n",
        "   BUCKET_NAME=\"text-classifier-model-$(date +%s)\"\n",
        "   aws s3 mb s3://$BUCKET_NAME\n",
        "   aws s3 cp model/text_classifier.pkl s3://$BUCKET_NAME/model/text_classifier.pkl\n",
        "   ```\n",
        "\n",
        "3. **Deploy with AWS SAM**:\n",
        "   ```bash\n",
        "   sam build\n",
        "   sam deploy --guided --parameter-overrides ModelBucketName=$BUCKET_NAME\n",
        "   ```\n",
        "\n",
        "4. **Test the API**:\n",
        "   ```bash\n",
        "   curl -X POST \"https://YOUR_API_ID.execute-api.REGION.amazonaws.com/Prod/classify\" \\\\\n",
        "     -H \"Content-Type: application/json\" \\\\\n",
        "     -d '{\"text\": \"Sample text for classification with enough content\"}'\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”— API Endpoint\n",
        "\n",
        "**POST** `/classify`\n",
        "\n",
        "**Request:**\n",
        "```json\n",
        "{\"text\": \"Your text here (min 10 characters)\"}\n",
        "```\n",
        "\n",
        "**Response (200):**\n",
        "```json\n",
        "{\n",
        "  \"id\": \"550e8400-e29b-41d4-a716-446655440000\",\n",
        "  \"category\": \"comp.graphics\",\n",
        "  \"confidence\": 0.9234,\n",
        "  \"input_length\": 78\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š Categories\n",
        "\n",
        "The pre-trained model classifies text into 4 categories from 20newsgroups:\n",
        "- `comp.graphics` - Computer graphics topics\n",
        "- `sci.med` - Medical science topics\n",
        "- `rec.sport.baseball` - Baseball/sports topics  \n",
        "- `talk.politics.mideast` - Political discussions\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“„ File Locations\n",
        "\n",
        "All project files are in: `/content/text-classification-service/`\n",
        "\n",
        "You can download the entire folder from Google Colab's file browser or use:\n",
        "```bash\n",
        "!cp -r text-classification-service /gdrive/My\\ Drive/  # if Google Drive is mounted\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Ready for AWS deployment! Follow DEPLOYMENT_GUIDE.md for complete instructions.**"
      ],
      "metadata": {
        "id": "yKa_L7FccQRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify all files created\n",
        "import os\n",
        "import json\n",
        "\n",
        "project_path = '/content/text-classification-service'\n",
        "\n",
        "# List all files\n",
        "files = []\n",
        "for root, dirs, filenames in os.walk(project_path):\n",
        "    for filename in filenames:\n",
        "        filepath = os.path.join(root, filename)\n",
        "        rel_path = os.path.relpath(filepath, project_path)\n",
        "        size = os.path.getsize(filepath)\n",
        "        files.append({\n",
        "            'path': rel_path,\n",
        "            'size_bytes': size,\n",
        "            'size_kb': round(size / 1024, 2)\n",
        "        })\n",
        "\n",
        "files.sort(key=lambda x: x['path'])\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"PROJECT STRUCTURE: text-classification-service\")\n",
        "print(f\"Total files created: {len(files)}\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "for f in files:\n",
        "    print(f\"{f['path']:50s} {f['size_bytes']:>8d} bytes ({f['size_kb']:>6.2f} KB)\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "total_size = sum(f['size_bytes'] for f in files)\n",
        "print(f\"Total project size: {total_size} bytes ({round(total_size/1024, 2)} KB)\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(\"\\nâœ… All files created successfully!\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Download the project folder from Colab's file browser\")\n",
        "print(\"2. Follow DEPLOYMENT_GUIDE.md for AWS deployment instructions\")\n",
        "print(\"3. Train model: python scripts/train_model.py\")\n",
        "print(\"4. Create S3 bucket and deploy with AWS SAM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-XZXTfZcX66",
        "outputId": "0f046d72-bdb6-4a85-b642-a327244b0c86"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "PROJECT STRUCTURE: text-classification-service\n",
            "Total files created: 16\n",
            "======================================================================\n",
            "\n",
            ".gitignore                                              408 bytes (  0.40 KB)\n",
            "CURL_EXAMPLES.sh                                       2221 bytes (  2.17 KB)\n",
            "DEPLOYMENT_GUIDE.md                                    4608 bytes (  4.50 KB)\n",
            "PROJECT_STRUCTURE.txt                                  7925 bytes (  7.74 KB)\n",
            "README.md                                              3475 bytes (  3.39 KB)\n",
            "requirements.txt                                         65 bytes (  0.06 KB)\n",
            "scripts/train_model.py                                 1162 bytes (  1.13 KB)\n",
            "src/__pycache__/lambda_handler.cpython-312.pyc         4259 bytes (  4.16 KB)\n",
            "src/__pycache__/logger.cpython-312.pyc                  754 bytes (  0.74 KB)\n",
            "src/__pycache__/model_loader.cpython-312.pyc           1138 bytes (  1.11 KB)\n",
            "src/lambda_handler.py                                  2754 bytes (  2.69 KB)\n",
            "src/logger.py                                           337 bytes (  0.33 KB)\n",
            "src/model_loader.py                                     633 bytes (  0.62 KB)\n",
            "swagger.yaml                                           2630 bytes (  2.57 KB)\n",
            "template.yaml                                          1948 bytes (  1.90 KB)\n",
            "tests/test_handler.py                                  1111 bytes (  1.08 KB)\n",
            "\n",
            "======================================================================\n",
            "Total project size: 35428 bytes (34.6 KB)\n",
            "======================================================================\n",
            "\n",
            "âœ… All files created successfully!\n",
            "\n",
            "Next steps:\n",
            "1. Download the project folder from Colab's file browser\n",
            "2. Follow DEPLOYMENT_GUIDE.md for AWS deployment instructions\n",
            "3. Train model: python scripts/train_model.py\n",
            "4. Create S3 bucket and deploy with AWS SAM\n"
          ]
        }
      ]
    }
  ]
}